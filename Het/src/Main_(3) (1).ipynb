{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2273f150",
      "metadata": {
        "id": "2273f150"
      },
      "outputs": [],
      "source": [
        "%pip install matplotlib seaborn numpy pandas nibabel nilearn scikit-learn statsmodels torch joblib networkx python-louvain\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "from nilearn.input_data import NiftiLabelsMasker\n",
        "import statsmodels.formula.api as smf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import re\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from joblib import Parallel, delayed\n",
        "%pip install scipy statsmodels\n",
        "from scipy.stats import shapiro, levene\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import networkx as nx\n",
        "import community.community_louvain as community_louvain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11571de7",
      "metadata": {
        "id": "11571de7"
      },
      "outputs": [],
      "source": [
        "data_dir = '/Users/hetjivani/NIAR_data/raw_data'\n",
        "labels_csv = '/Users/hetjivani/NIAR/Neuro-Imaging-Analysis-and-Research/GroupE/LatSim/demo.csv'\n",
        "years = [1, 2, 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2297a2b3",
      "metadata": {
        "id": "2297a2b3"
      },
      "outputs": [],
      "source": [
        "from nilearn import datasets\n",
        "\n",
        "# Fetch the Schaefer 2018 atlas (e.g., 100 parcels, 7 networks)\n",
        "schaefer = datasets.fetch_atlas_schaefer_2018(n_rois=100, yeo_networks=7, resolution_mm=1)\n",
        "\n",
        "# Initialize masker with Schaefer maps\n",
        "data_masker = NiftiLabelsMasker(labels_img=schaefer.maps, standardize=True)  # human‐readable labels for each parcel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b85f15fb",
      "metadata": {
        "id": "b85f15fb"
      },
      "outputs": [],
      "source": [
        "# Function to compute network metrics\n",
        "def compute_network_measures(fc_matrix):\n",
        "    G = nx.from_numpy_array(fc_matrix)\n",
        "\n",
        "    degree_centrality = np.mean(list(nx.degree_centrality(G).values()))\n",
        "    global_efficiency = nx.global_efficiency(G)\n",
        "    clustering_coeff = nx.average_clustering(G, weight='weight')\n",
        "    partition = community_louvain.best_partition(G, weight='weight')\n",
        "    modularity = community_louvain.modularity(partition, G, weight='weight')\n",
        "\n",
        "    return degree_centrality, global_efficiency, clustering_coeff, modularity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85897aa3",
      "metadata": {
        "id": "85897aa3"
      },
      "outputs": [],
      "source": [
        "records = []\n",
        "pattern = re.compile(r\"Denoised_([pP]?\\d{2}|C\\d{2})_rs-(\\d+)_MNI.*\\.nii(?:\\.gz)?$\")\n",
        "for filepath in glob.glob(os.path.join(data_dir, 'Denoised_*_rs-*_MNI.nii*')):\n",
        "    fname = os.path.basename(filepath)\n",
        "    m = pattern.match(fname)\n",
        "    if not m:\n",
        "        continue\n",
        "\n",
        "    subj, rs_year = m.groups()\n",
        "    year = int(rs_year)\n",
        "    if year not in years:\n",
        "        continue\n",
        "\n",
        "    # Load and extract time-series\n",
        "    img = nib.load(filepath)\n",
        "    ts = data_masker.fit_transform(img)\n",
        "\n",
        "    # Compute static FC (Pearson correlation)\n",
        "    corr = np.corrcoef(ts.T)\n",
        "\n",
        "    # Set negative correlations to zero for network analysis\n",
        "    corr_net = np.where(corr < 0, 0, corr)\n",
        "\n",
        "    # Vectorize upper triangle\n",
        "    triu_idx = np.triu_indices_from(corr, k=1)\n",
        "    fc_vec = corr[triu_idx]\n",
        "\n",
        "    # Compute network metrics\n",
        "    degree_cent, glob_eff, cluster_coeff, mod = compute_network_measures(corr_net)\n",
        "\n",
        "    # Dynamic FC variance (example calculation, adjust if sliding windows exist)\n",
        "    dFC_variance = np.var(corr)\n",
        "\n",
        "    records.append((subj, year, fc_vec, degree_cent, glob_eff, cluster_coeff, mod, dFC_variance))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12fca88c",
      "metadata": {
        "id": "12fca88c"
      },
      "outputs": [],
      "source": [
        "# Unpack results\n",
        "subjects, yrs, fc_vecs, degrees, efficiencies, clusterings, modularities, dFC_vars = zip(*records)\n",
        "\n",
        "fc_array = np.vstack(fc_vecs)\n",
        "df = pd.DataFrame(fc_array, columns=[f'f{i}' for i in range(fc_array.shape[1])])\n",
        "\n",
        "# Add metadata and covariates\n",
        "df['subject'] = subjects\n",
        "df['year'] = yrs\n",
        "df['degree_centrality'] = degrees\n",
        "df['global_efficiency'] = efficiencies\n",
        "df['clustering_coeff'] = clusterings\n",
        "df['modularity'] = modularities\n",
        "df['dFC_variance'] = dFC_vars\n",
        "\n",
        "# Merge in labels\n",
        "labels_df = pd.read_csv(labels_csv)\n",
        "labels_df['label_enc'] = LabelEncoder().fit_transform(labels_df['Status'])\n",
        "\n",
        "df = df.merge(labels_df[['SubId', 'label_enc']], left_on='subject', right_on='SubId')\n",
        "df.drop(columns=['SubId'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "532e3c52",
      "metadata": {
        "id": "532e3c52"
      },
      "outputs": [],
      "source": [
        "print(df.head())\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9f6f5ed",
      "metadata": {
        "id": "f9f6f5ed"
      },
      "outputs": [],
      "source": [
        "def compute_lme_residuals(df, feature_cols, covariates=None):\n",
        "    df_out = df.copy()\n",
        "    for feat in feature_cols:\n",
        "        if covariates:\n",
        "            formula = f\"{feat} ~ year + \" + \" + \".join(covariates)\n",
        "        else:\n",
        "            formula = f\"{feat} ~ year\"\n",
        "\n",
        "        try:\n",
        "            md = smf.mixedlm(formula, df_out, groups=df_out['subject'], re_formula=\"~year\")\n",
        "            mdf = md.fit()\n",
        "            resid_col = f\"{feat}_resid\"\n",
        "            df_out[resid_col] = df_out[feat] - mdf.fittedvalues\n",
        "        except Exception as e:\n",
        "            print(f\"Model failed for feature {feat}: {e}\")\n",
        "            continue\n",
        "    return df_out\n",
        "\n",
        "# Define feature columns (FC edges) and covariates (network measures)\n",
        "feature_cols = [c for c in df.columns if c.startswith('f')]\n",
        "covariate_cols = ['degree_centrality', 'global_efficiency', 'clustering_coeff', 'modularity', 'dFC_variance']\n",
        "\n",
        "# Compute residuals while controlling for covariates\n",
        "df_resid = compute_lme_residuals(df, feature_cols, covariates=covariate_cols)\n",
        "\n",
        "# Prepare residual feature matrix and labels\n",
        "resid_cols = [f + '_resid' for f in feature_cols]\n",
        "X_resid = df_resid[resid_cols].values  # shape: (n_samples, n_edges)\n",
        "y = df_resid['label_enc'].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20eb051d",
      "metadata": {
        "id": "20eb051d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5e5a77d3",
      "metadata": {
        "id": "5e5a77d3"
      },
      "source": [
        "*** LME Test ***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bab3c90",
      "metadata": {
        "id": "6bab3c90"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as ss\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from typing import Tuple, List, Optional\n",
        "from statsmodels.regression.mixed_linear_model import MixedLMResultsWrapper\n",
        "from scipy.stats import shapiro, levene\n",
        "\n",
        "\n",
        "def run_edge_diagnostics(df,\n",
        "                         feature: str,\n",
        "                         covariates: Optional[List[str]] = None,\n",
        "                         alpha: float = 0.05,\n",
        "                         plot: bool = True):\n",
        "    \"\"\"\n",
        "    Evaluates LME model for a given feature (FC edge) with diagnostics.\n",
        "\n",
        "    Parameters:\n",
        "    - df : DataFrame with subject, feature, year, and optional covariates\n",
        "    - feature : column name of FC edge\n",
        "    - covariates : list of covariate names (e.g., network metrics)\n",
        "    - alpha : significance threshold\n",
        "    - plot : whether to generate diagnostic plots\n",
        "\n",
        "    Returns:\n",
        "    - dictionary with test results\n",
        "    \"\"\"\n",
        "\n",
        "    # Construct formula\n",
        "    fixed_effects = \"year\"\n",
        "    if covariates:\n",
        "        fixed_effects += \" + \" + \" + \".join(covariates)\n",
        "\n",
        "    f_alt = f\"{feature} ~ {fixed_effects}\"\n",
        "    f_null = f\"{feature} ~ 1\"\n",
        "\n",
        "    try:\n",
        "        alt = smf.mixedlm(f_alt, df, groups=df[\"subject\"], re_formula=\"~year\").fit(reml=False, method=\"lbfgs\")\n",
        "        null = smf.mixedlm(f_null, df, groups=df[\"subject\"], re_formula=\"1\").fit(reml=False, method=\"lbfgs\")\n",
        "    except Exception as e:\n",
        "        print(f\"{feature}: model fitting error → {e}\")\n",
        "        return None\n",
        "\n",
        "    if np.isnan(alt.llf) or np.isnan(null.llf):\n",
        "        print(f\"{feature}: model did not converge; skipping\")\n",
        "        return None\n",
        "\n",
        "    # Likelihood Ratio Test\n",
        "    lr = 2 * (alt.llf - null.llf)\n",
        "    dfd = alt.df_modelwc - null.df_modelwc\n",
        "    p_lr = ss.chi2.sf(lr, dfd)\n",
        "\n",
        "    # R² Calculation\n",
        "    var_f = np.var(alt.fittedvalues)\n",
        "    var_re = np.trace(alt.cov_re)\n",
        "    var_e = alt.scale\n",
        "    r2_marg = var_f / (var_f + var_re + var_e)\n",
        "    r2_cond = (var_f + var_re) / (var_f + var_re + var_e)\n",
        "\n",
        "    # Residuals\n",
        "    resid = alt.resid\n",
        "    fitted = alt.fittedvalues\n",
        "\n",
        "    # Shapiro-Wilk Test (Normality)\n",
        "    _, p_shapiro = shapiro(resid)\n",
        "\n",
        "    # Levene’s Test (Homoscedasticity between groups)\n",
        "    if 'group' in df.columns:\n",
        "        g1 = resid[df[\"group\"] == 'control']\n",
        "        g2 = resid[df[\"group\"] == 'patient']\n",
        "        _, p_levene = levene(g1, g2)\n",
        "    else:\n",
        "        p_levene = np.nan\n",
        "\n",
        "    # Plots\n",
        "    if plot:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
        "        sm.qqplot(resid, line='s', ax=axes[0])\n",
        "        axes[0].set_title(\"QQ plot\")\n",
        "        axes[1].scatter(fitted, resid, s=10)\n",
        "        axes[1].axhline(0, ls='--')\n",
        "        axes[1].set_title(\"Residual vs Fitted\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Output\n",
        "    # print(f\"=== Edge {feature} ===\")\n",
        "    # print(f\"AIC null  = {null.aic:.2f}\")\n",
        "    # print(f\"AIC alt   = {alt.aic:.2f}\")\n",
        "    # print(f\"LR χ² = {lr:.2f} (df={dfd})  p = {p_lr:.3g}\")\n",
        "    # print(f\"R² marginal = {r2_marg:.3f},  conditional = {r2_cond:.3f}\")\n",
        "    # print(f\"Shapiro-Wilk p = {p_shapiro:.4f}, Levene’s p = {p_levene:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'feature': feature,\n",
        "        'lr': lr,\n",
        "        'p_lr': p_lr,\n",
        "        'r2_marg': r2_marg,\n",
        "        'r2_cond': r2_cond,\n",
        "        'p_shapiro': p_shapiro,\n",
        "        'p_levene': p_levene,\n",
        "        'converged': alt.converged\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a07b0e6e",
      "metadata": {
        "id": "a07b0e6e"
      },
      "outputs": [],
      "source": [
        "covariate_cols = ['degree_centrality', 'global_efficiency', 'dFC_variance']  # adapt as needed\n",
        "results = []\n",
        "\n",
        "for feat in [c for c in df.columns if c.startswith('f')]:\n",
        "    res = run_edge_diagnostics(df, feature=feat, covariates=covariate_cols, plot=False)\n",
        "    if res:\n",
        "        results.append(res)\n",
        "\n",
        "df_eval = pd.DataFrame(results)\n",
        "Evaulate = df_eval.to_csv('edge_diagnostics_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de0ce0a6",
      "metadata": {
        "id": "de0ce0a6"
      },
      "outputs": [],
      "source": [
        "feature = feature_cols[0]\n",
        "alt_model = smf.mixedlm(f\"{feature} ~ year\", df, groups=df[\"subject\"], re_formula=\"~year\").fit(reml=False, method=\"lbfgs\")\n",
        "print(alt_model.cov_re)      # 2×2 matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6840a13e",
      "metadata": {
        "id": "6840a13e"
      },
      "outputs": [],
      "source": [
        "def fit_edge(df, col, covariates=None):\n",
        "    \"\"\"\n",
        "    Fit a random-intercept LME for edge `col`, return slope β1 and p-value.\n",
        "\n",
        "    Parameters:\n",
        "    - df : pd.DataFrame with subject, edge, year, and optionally covariates\n",
        "    - col : feature/column name to fit (e.g. 'f14')\n",
        "    - covariates : list of additional fixed-effect covariates (optional)\n",
        "\n",
        "    Returns:\n",
        "    - dict with edge, beta (slope for year), and p-value\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Construct formula\n",
        "        fixed_effects = \"year\"\n",
        "        if covariates:\n",
        "            fixed_effects += \" + \" + \" + \".join(covariates)\n",
        "\n",
        "        formula = f\"{col} ~ {fixed_effects}\"\n",
        "        model = smf.mixedlm(formula, df, groups=df[\"subject\"], re_formula=\"1\")\n",
        "        result = model.fit(reml=False, method=\"lbfgs\")\n",
        "\n",
        "        beta = result.params[\"year\"]\n",
        "        pval = result.pvalues[\"year\"]\n",
        "        return {\"edge\": col, \"beta\": beta, \"p\": pval}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fit model for {col}: {e}\")\n",
        "        return {\"edge\": col, \"beta\": np.nan, \"p\": np.nan}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "436d425e",
      "metadata": {
        "id": "436d425e"
      },
      "outputs": [],
      "source": [
        "covariates = ['degree_centrality', 'global_efficiency', 'dFC_variance']\n",
        "edge_results = [fit_edge(df, col, covariates=covariates) for col in df.columns if col.startswith('f')]\n",
        "\n",
        "df_edges = pd.DataFrame(edge_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00fdee36",
      "metadata": {
        "id": "00fdee36"
      },
      "outputs": [],
      "source": [
        "top5 = stats_df.nsmallest(5, \"p\").reset_index(drop=True)\n",
        "print(top5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d360878",
      "metadata": {
        "id": "6d360878"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 5, figsize=(18, 3), sharey=True)\n",
        "\n",
        "for ax, (_, row) in zip(axes, top5.iterrows()):\n",
        "    col  = row[\"edge\"]\n",
        "    beta = row[\"beta\"]\n",
        "    pval = row[\"p\"]\n",
        "\n",
        "    # Fit the model for this edge\n",
        "    model = smf.mixedlm(f\"{col} ~ year\", df, groups=df[\"subject\"], re_formula=\"1\")\n",
        "    res = model.fit(reml=False, method=\"lbfgs\")\n",
        "\n",
        "    # scatter raw data\n",
        "    ax.scatter(df[\"year\"], df[col], s=10, alpha=0.6)\n",
        "\n",
        "    # population trend line\n",
        "    x_line = np.array([df[\"year\"].min(), df[\"year\"].max()])\n",
        "    y_line = res.params[\"Intercept\"] + beta * x_line\n",
        "    ax.plot(x_line, y_line, lw=2, color=\"red\")\n",
        "\n",
        "    ax.set_title(f\"{col}\\nβ₁={beta:.3g}, p={pval:.2g}\")\n",
        "    ax.set_xlabel(\"Year\")\n",
        "\n",
        "axes[0].set_ylabel(\"Edge value (Fisher-z)\")\n",
        "fig.suptitle(\"Top-5 edges with strongest linear trend\", y=1.05, fontsize=14)\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "787e890d",
      "metadata": {
        "id": "787e890d"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "from scipy.stats import shapiro, chi2\n",
        "\n",
        "def evaluate_lme_feature(df, feature, covariates=['year']):\n",
        "    \"\"\"\n",
        "    Fit random-intercept + slope LME for a feature with covariates,\n",
        "    and return diagnostics (Shapiro-Wilk, LR test, AIC, BIC).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        formula = f\"{feature} ~ {' + '.join(covariates)}\"\n",
        "\n",
        "        # Full model: random intercept + slope\n",
        "        md_full = smf.mixedlm(formula, df, groups=df['subject'], re_formula=\"~year\")\n",
        "        mdf = md_full.fit(method='lbfgs')\n",
        "\n",
        "        # Null model: random intercept only\n",
        "        md_null = smf.mixedlm(formula, df, groups=df['subject'], re_formula=\"1\")\n",
        "        mdf_null = md_null.fit(method='lbfgs')\n",
        "\n",
        "        # Manual Likelihood Ratio Test\n",
        "        lr_stat = 2 * (mdf.llf - mdf_null.llf)\n",
        "        df_diff = mdf.df_modelwc - mdf_null.df_modelwc\n",
        "        p_lr = chi2.sf(lr_stat, df_diff)\n",
        "\n",
        "        # Residual diagnostics\n",
        "        residuals = mdf.resid\n",
        "        _, p_shapiro = shapiro(residuals)\n",
        "\n",
        "        return {\n",
        "            'feature': feature,\n",
        "            'shapiro_p': p_shapiro,\n",
        "            'lr_p': p_lr,\n",
        "            'AIC_full': mdf.aic,\n",
        "            'BIC_full': mdf.bic,\n",
        "            'converged': mdf.converged\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{feature}: model fitting failed → {e}\")\n",
        "        return {\n",
        "            'feature': feature,\n",
        "            'shapiro_p': None,\n",
        "            'lr_p': None,\n",
        "            'AIC_full': None,\n",
        "            'BIC_full': None,\n",
        "            'converged': False\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "645191da",
      "metadata": {
        "id": "645191da"
      },
      "outputs": [],
      "source": [
        "covs = ['year', 'degree_centrality', 'global_efficiency', 'dFC_variance']\n",
        "results = [evaluate_lme_feature(df, f, covariates=covs) for f in df.columns if f.startswith('f')]\n",
        "df_diagnostics = pd.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "016d9032",
      "metadata": {
        "id": "016d9032"
      },
      "outputs": [],
      "source": [
        "# Sort by normality issues\n",
        "non_normal = df_diagnostics[df_diagnostics['shapiro_p'] < 0.05]\n",
        "print(f\"Number of features violating normality: {len(non_normal)}\")\n",
        "\n",
        "# # Sort by homoscedasticity issues\n",
        "# heteroscedastic = df_diagnostics[df_diagnostics['levene_p'] < 0.05]\n",
        "# print(f\"Number of features violating homoscedasticity: {len(heteroscedastic)}\")\n",
        "\n",
        "# Check random-effects significance\n",
        "significant_random_effects = df_diagnostics[df_diagnostics['lr_p'] < 0.05]\n",
        "print(f\"Features benefiting from random slopes: {len(significant_random_effects)}\")\n",
        "\n",
        "# Check convergence issues\n",
        "convergence_issues = df_diagnostics[~df_diagnostics['converged']]\n",
        "print(f\"Features with convergence issues: {len(convergence_issues)}\")\n",
        "\n",
        "df_diagnostics.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "318bcc88",
      "metadata": {
        "id": "318bcc88"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2df8e756",
      "metadata": {
        "id": "2df8e756"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
        "\n",
        "sns.histplot(df_diagnostics['shapiro_p'], bins=20, ax=axs[0])\n",
        "axs[0].axvline(0.05, color='red', linestyle='--')\n",
        "axs[0].set_title('Distribution of Shapiro-Wilk p-values (Normality)')\n",
        "\n",
        "# sns.histplot(results_df['levene_p'], bins=20, ax=axs[1])\n",
        "# axs[1].axvline(0.05, color='red', linestyle='--')\n",
        "# axs[1].set_title('Distribution of Levene’s test p-values (Homoscedasticity)')\n",
        "\n",
        "sns.histplot(df_diagnostics['lr_p'], bins=20, ax=axs[2])\n",
        "axs[2].axvline(0.05, color='red', linestyle='--')\n",
        "axs[2].set_title('Distribution of LR-test p-values (Random Effects)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd645d21",
      "metadata": {
        "id": "fd645d21"
      },
      "source": [
        "#Sanity Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a90217a2",
      "metadata": {
        "id": "a90217a2"
      },
      "outputs": [],
      "source": [
        "def fit_and_resid(df, col):\n",
        "    \"\"\"Random-intercept-only LME; return residual Series + slope stats.\"\"\"\n",
        "    res = smf.mixedlm(f\"{col} ~ year\", df,\n",
        "                      groups=df[\"subject\"], re_formula=\"1\"\n",
        "                     ).fit(reml=False, method=\"lbfgs\")\n",
        "    series = res.resid.rename(f\"{col}_resid\")\n",
        "    return series, res.params[\"year\"], res.pvalues[\"year\"]\n",
        "\n",
        "# --- parallel loop over all edges -------------------------------------------\n",
        "out   = Parallel(n_jobs=-1)(\n",
        "          delayed(fit_and_resid)(df, c) for c in feature_cols\n",
        "       )\n",
        "\n",
        "resid_series, betas, pvals = zip(*out)          # unzip tuples\n",
        "df_resid  = pd.concat([df] + list(resid_series), axis=1, copy=False)\n",
        "\n",
        "# tidy stats table\n",
        "stats_df  = pd.DataFrame({\n",
        "    \"edge\": feature_cols,\n",
        "    \"beta\": betas,\n",
        "    \"p\"   : pvals\n",
        "}).dropna()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54d0f5d7",
      "metadata": {
        "id": "54d0f5d7"
      },
      "outputs": [],
      "source": [
        "resid_cols = [f\"{c}_resid\" for c in feature_cols]\n",
        "sd = df_resid[resid_cols].std()\n",
        "\n",
        "plt.figure(figsize=(5,3))\n",
        "sd.hist(bins=50); plt.title(\"Residual SD per edge\"); plt.xlabel(\"σ\"); plt.ylabel(\"count\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a26b8d9",
      "metadata": {
        "id": "7a26b8d9"
      },
      "outputs": [],
      "source": [
        "thr = 0.05\n",
        "keep_mask   = sd > thr            # tune threshold\n",
        "keep_edges  = sd[keep_mask].index\n",
        "print(f\"Keeping {len(keep_edges)} / {len(resid_cols)} edges \"\n",
        "      f\"({100*keep_mask.mean():.1f}% ) with SD > {thr}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b05721",
      "metadata": {
        "id": "b2b05721"
      },
      "outputs": [],
      "source": [
        "X_resid = df_resid[keep_edges].astype(\"float32\").to_numpy()   # (samples × kept_edges)\n",
        "y       = df_resid[\"label_enc\"].values\n",
        "print(\"Residual matrix shape:\", X_resid.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc2c6ac5",
      "metadata": {
        "id": "bc2c6ac5"
      },
      "outputs": [],
      "source": [
        "def windowed_fc(ts, win_len=40, step=20):\n",
        "    \"\"\"\n",
        "    ts : ndarray shape (T, N)  -- T time points, N ROIs\n",
        "    Returns ndarray  (W, N, N) where W is number of windows.\n",
        "    \"\"\"\n",
        "    T, N = ts.shape\n",
        "    if T < win_len:\n",
        "        return np.empty((0, N, N), dtype=np.float32)\n",
        "\n",
        "    windows = []\n",
        "    for start in range(0, T - win_len + 1, step):\n",
        "        seg = ts[start:start + win_len]          # win_len × N\n",
        "        c   = np.corrcoef(seg.T)                 # N × N Pearson-r\n",
        "        windows.append(np.arctanh(np.clip(c, -0.9999, 0.9999)))  # Fisher-z\n",
        "    return np.stack(windows, axis=0)             # W × N × N\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "210c8032",
      "metadata": {
        "id": "210c8032"
      },
      "outputs": [],
      "source": [
        "sid   = df[\"subject\"].sample(1).iloc[0]\n",
        "rows  = df_resid[\"subject\"] == sid\n",
        "ts    = df_resid.loc[rows, list(keep_edges)].to_numpy()\n",
        "fc_ws = windowed_fc(ts)\n",
        "\n",
        "print(\"Window-wise FC std:\", fc_ws.std())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78dbc855",
      "metadata": {
        "id": "78dbc855"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "gkf   = GroupKFold(n_splits=5)\n",
        "aucs  = []\n",
        "\n",
        "for train, test in gkf.split(X_resid, y, groups=df_resid[\"subject\"]):\n",
        "    clf = LogisticRegression(max_iter=500, n_jobs=-1)\n",
        "    clf.fit(X_resid[train], y[train])\n",
        "    aucs.append(roc_auc_score(y[test], clf.predict_proba(X_resid[test])[:,1]))\n",
        "\n",
        "print(\"Log-Reg ROC-AUC:\", np.mean(aucs), \"±\", np.std(aucs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8608d5e",
      "metadata": {
        "id": "d8608d5e"
      },
      "outputs": [],
      "source": [
        "# Option 1: Save as DataFrame with column names\n",
        "pd.DataFrame(X_resid, columns=keep_edges).to_csv(\"output.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a1f8017",
      "metadata": {
        "id": "5a1f8017"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "#  Dynamic FC extractor  –  from 4-D NIfTI → sliding-window FC tensors\n",
        "# =============================================================================\n",
        "# CONFIG (edit if your paths or atlas differ)\n",
        "DATA_DIR      = \"/Users/hetjivani/NIAR_data/raw_data\"            # folder with your *.nii / *.nii.gz\n",
        "OUT_DIR       = \"/Users/hetjivani/NIAR/Neuro-Imaging-Analysis-and-Research/GroupE/LatSim/Dynamic_Fc\"       # where the tensors will live\n",
        "ATLAS_N_ROI   = 100                          # Schaefer granularity; or 400, 100 …\n",
        "WIN_LEN       = 40                           # window length in TRs (e.g. 40 × 2 s = 80 s)\n",
        "STEP          = 20                           # stride in TRs\n",
        "CLIP_R        = 0.9999                       # avoid arctanh(±1)\n",
        "N_JOBS        = -1                           # use all CPU cores\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "import os, glob, pathlib, numpy as np, pandas as pd\n",
        "from joblib import Parallel, delayed\n",
        "from nilearn import datasets, masking\n",
        "from nilearn.input_data import NiftiLabelsMasker\n",
        "\n",
        "# 1 ── Grab atlas once\n",
        "schaefer = datasets.fetch_atlas_schaefer_2018(n_rois=ATLAS_N_ROI, yeo_networks=7)\n",
        "masker   = NiftiLabelsMasker(labels_img=schaefer.maps,\n",
        "                             standardize=True, detrend=True, low_pass=0.08, high_pass=0.01,\n",
        "                             t_r=2.0)                        # ← set t_r to your actual TR in seconds\n",
        "\n",
        "# 2 ── Helper: sliding-window FC on an ROI time-series matrix\n",
        "def windowed_fc(ts, win=WIN_LEN, step=STEP, clip=CLIP_R):\n",
        "    T, N = ts.shape\n",
        "    if T < win:\n",
        "        return np.zeros((0, N, N), dtype=np.float32)\n",
        "    wins = []\n",
        "    for start in range(0, T - win + 1, step):\n",
        "        seg = ts[start:start + win]                          # win × N\n",
        "        c   = np.corrcoef(seg.T)\n",
        "        wins.append(np.arctanh(np.clip(c, -clip, clip)))     # Fisher-z\n",
        "    return np.stack(wins).astype(\"float32\")                  # W × N × N\n",
        "\n",
        "# 3 ── Worker for one file\n",
        "def process_nifti(nii_path):\n",
        "    sid   = pathlib.Path(nii_path).stem.split(\"_\")[0]        # customise to your naming\n",
        "    ts    = masker.fit_transform(nii_path)                   # T × N\n",
        "    dFC   = windowed_fc(ts)\n",
        "    if dFC.shape[0] == 0:\n",
        "        return None\n",
        "    out_f = pathlib.Path(OUT_DIR) / f\"{sid}_dFC.npy\"\n",
        "    np.save(out_f, dFC)\n",
        "    return {\"subject\": sid, \"T\": ts.shape[0], \"W\": dFC.shape[0], \"file\": str(out_f)}\n",
        "\n",
        "# 4 ── Main loop (parallel)\n",
        "pathlib.Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "nii_files = glob.glob(os.path.join(DATA_DIR, \"*.nii*\"))\n",
        "meta_rows = Parallel(n_jobs=N_JOBS, verbose=5)(\n",
        "    delayed(process_nifti)(p) for p in nii_files\n",
        ")\n",
        "meta_df = pd.DataFrame([m for m in meta_rows if m is not None])\n",
        "meta_df.to_csv(os.path.join(OUT_DIR, \"index.csv\"), index=False)\n",
        "\n",
        "print(\"✓ Finished.  Tensor files in\", OUT_DIR)\n",
        "print(meta_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64a6baed",
      "metadata": {
        "id": "64a6baed"
      },
      "outputs": [],
      "source": [
        "DYNAMIC_FC_DIR = \"/Users/hetjivani/NIAR/Neuro-Imaging-Analysis-and-Research/GroupE/LatSim/Dynamic_Fc\"\n",
        "MANIFEST_CSV   = os.path.join(DYNAMIC_FC_DIR, \"index.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "995a14cb",
      "metadata": {
        "id": "995a14cb"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(MANIFEST_CSV):\n",
        "    meta_df = pd.read_csv(MANIFEST_CSV)\n",
        "else:\n",
        "    files = sorted(glob.glob(os.path.join(DYNAMIC_FC_DIR, \"*_dFC.npy\")))\n",
        "    meta_df = pd.DataFrame({\n",
        "        \"subject\": [pathlib.Path(f).stem.split(\"_\")[0] for f in files],\n",
        "        \"file\"   : files\n",
        "    })\n",
        "    # Optional: count windows T if you like\n",
        "    meta_df.to_csv(MANIFEST_CSV, index=False)\n",
        "    print(f\"Manifest written → {MANIFEST_CSV}\")\n",
        "\n",
        "print(meta_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c2f544c",
      "metadata": {
        "id": "7c2f544c"
      },
      "outputs": [],
      "source": [
        "def vec_upper(mat: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Return upper-triangle (k=1) of a square matrix as 1-D vector.\"\"\"\n",
        "    return mat[np.triu_indices_from(mat, k=1)]\n",
        "\n",
        "sub_stats = []\n",
        "for _, row in meta_df.iterrows():\n",
        "    dFC  = np.load(row[\"file\"])          # (W, N, N)\n",
        "    W, N = dFC.shape[0], dFC.shape[1]\n",
        "    if W == 0:\n",
        "        continue\n",
        "    # stack window vectors  →  (W, n_edges)\n",
        "    edge_vecs = np.stack([vec_upper(w) for w in dFC])\n",
        "    edge_sd   = edge_vecs.std(axis=0)    # variability per edge\n",
        "    sub_stats.append({\n",
        "        \"subject\"      : row[\"subject\"],\n",
        "        \"n_windows\"    : W,\n",
        "        \"mean_edge_sd\" : edge_sd.mean(),\n",
        "        \"max_edge_sd\"  : edge_sd.max()\n",
        "    })\n",
        "\n",
        "sub_df = pd.DataFrame(sub_stats)\n",
        "print(\"\\n=== Per-subject dynamic variability ===\")\n",
        "print(sub_df.sort_values(\"mean_edge_sd\", ascending=False).head())\n",
        "sub_df.to_csv(os.path.join(DYNAMIC_FC_DIR, \"subject_variability.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad1f6841",
      "metadata": {
        "id": "ad1f6841"
      },
      "outputs": [],
      "source": [
        "all_vecs = []\n",
        "for _, row in meta_df.iterrows():\n",
        "    dFC = np.load(row[\"file\"])\n",
        "    all_vecs.append(np.stack([vec_upper(w) for w in dFC]))\n",
        "all_vecs = np.vstack(all_vecs)           # (total_W, n_edges)\n",
        "\n",
        "edge_sd_global = all_vecs.std(axis=0)\n",
        "# Map back to ROI indices\n",
        "N = dFC.shape[1]\n",
        "tri_i, tri_j   = np.triu_indices(N, k=1)\n",
        "top_idx        = np.argsort(edge_sd_global)[-10:][::-1]\n",
        "\n",
        "top10_df = pd.DataFrame({\n",
        "    \"ROI_i\"            : tri_i[top_idx],\n",
        "    \"ROI_j\"            : tri_j[top_idx],\n",
        "    \"SD_across_windows\": edge_sd_global[top_idx]\n",
        "})\n",
        "print(\"\\n=== Top-10 most dynamic edges ===\")\n",
        "print(top10_df)\n",
        "top10_df.to_csv(os.path.join(DYNAMIC_FC_DIR, \"top10_edges.csv\"), index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcb9c827",
      "metadata": {
        "id": "fcb9c827"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(edge_sd_global, bins=60, edgecolor=\"k\")\n",
        "plt.xlabel(\"Edge SD across windows\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of edge-wise dynamic variability\")\n",
        "plt.tight_layout()\n",
        "fig_path = os.path.join(DYNAMIC_FC_DIR, \"edge_variability_hist.png\")\n",
        "plt.savefig(fig_path, dpi=300)\n",
        "print(f\"\\nHistogram saved → {fig_path}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "367e6fbd",
      "metadata": {
        "id": "367e6fbd"
      },
      "source": [
        "VISULAISE THE TOP 10 DYNAMIC BRAIN REGIONS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8171849",
      "metadata": {
        "id": "f8171849"
      },
      "outputs": [],
      "source": [
        "TOP10_CSV = pathlib.Path(\"/Users/hetjivani/NIAR/Neuro-Imaging-Analysis-and-Research/GroupE/LatSim/Dynamic_Fc/top10_edges.csv\")          # produced by the previous script\n",
        "ATLAS_N_ROI = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4724f2cc",
      "metadata": {
        "id": "4724f2cc"
      },
      "outputs": [],
      "source": [
        "from nilearn import plotting\n",
        "\n",
        "atlas = datasets.fetch_atlas_schaefer_2018(n_rois=ATLAS_N_ROI, yeo_networks=7)\n",
        "node_coords = plotting.find_parcellation_cut_coords(labels_img=atlas['maps'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15ec3df5",
      "metadata": {
        "id": "15ec3df5"
      },
      "outputs": [],
      "source": [
        "top10 = pd.read_csv(TOP10_CSV)\n",
        "\n",
        "# Edge weights (for thickness / colour scale)\n",
        "edge_weights = top10[\"SD_across_windows\"].values\n",
        "edge_weights_norm = (edge_weights - edge_weights.min()) / (edge_weights.max() - edge_weights.min())\n",
        "\n",
        "# Build an empty adjacency matrix and fill the 10 edges\n",
        "N = ATLAS_N_ROI\n",
        "adj = np.zeros((N, N))\n",
        "for (i, row) in top10.iterrows():\n",
        "    adj[int(row[\"ROI_i\"]), int(row[\"ROI_j\"])] = edge_weights_norm[i]\n",
        "    adj[int(row[\"ROI_j\"]), int(row[\"ROI_i\"])] = edge_weights_norm[i]  # symmetric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d86e0c6d",
      "metadata": {
        "id": "d86e0c6d"
      },
      "outputs": [],
      "source": [
        "node_strength = adj.sum(axis=0)\n",
        "node_strength_norm = (node_strength - node_strength.min()) / (\n",
        "    node_strength.max() - node_strength.min() + 1e-6\n",
        ")\n",
        "\n",
        "# Map node strength → a dark-blue colour map (higher = darker)\n",
        "from matplotlib.cm import get_cmap\n",
        "cmap = get_cmap(\"Blues\")\n",
        "node_colors = [cmap(s) for s in node_strength_norm]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1939b705",
      "metadata": {
        "id": "1939b705"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "node_colors_arr = np.array(node_colors)  # Ensure correct shape for plot_connectome\n",
        "\n",
        "display = plotting.plot_connectome(\n",
        "    adj,\n",
        "    node_coords,\n",
        "    node_color=node_colors_arr,\n",
        "    edge_cmap=\"Reds\",\n",
        "    edge_vmin=0,\n",
        "    edge_vmax=1,\n",
        "    edge_threshold=\"0%\",      # draw all 10 edges\n",
        "    node_size=30 + 120 * node_strength_norm,  # emphasise important nodes\n",
        "    title=\"Top-10 Most Dynamic Edges (Schaefer-200)\",\n",
        ")\n",
        "fig_path = \"top10_dynamic_edges.png\"\n",
        "display.savefig(fig_path, dpi=300)\n",
        "display.close()\n",
        "print(f\"Saved visualisation to {fig_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6654efa8",
      "metadata": {
        "id": "6654efa8"
      },
      "source": [
        "# DAST GCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3NqvNZawoPHA",
      "metadata": {
        "id": "3NqvNZawoPHA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ------------------- Settings and Load Data -------------------\n",
        "TENSOR_PATH = \"/Users/hetjivani/NIAR/Neuro-Imaging-Analysis-and-Research/GroupE/LatSim/Dynamic_Fc/Denoised_dFC.npy\"\n",
        "dFC = np.load(TENSOR_PATH).astype(\"float32\")  # Load dynamic FC tensor with shape (windows, N, N)\n",
        "print(\"Shape of loaded dFC tensor:\", dFC.shape)\n",
        "W = dFC.shape[0]\n",
        "\n",
        "# Define train/test split percentages\n",
        "YEAR_SPLIT = [0.0, 0.66, 1.0]  # 0-66% for train, 66-100% for test\n",
        "w1, w2 = int(YEAR_SPLIT[1] * W), int(YEAR_SPLIT[2] * W)\n",
        "\n",
        "# Split data into train and test tensors\n",
        "train_tensor = dFC[:w1]                # Training data: first 66%\n",
        "# For test, include last SEQ_LEN_IN windows from train to allow sliding window input\n",
        "SEQ_LEN_IN = 5\n",
        "test_tensor = dFC[w1 - SEQ_LEN_IN : w2]\n",
        "\n",
        "print(\"Train tensor shape:\", train_tensor.shape)\n",
        "print(\"Test tensor shape:\", test_tensor.shape)\n",
        "\n",
        "# ------------------- Normalization (Important to Avoid Data Leakage) -------------------\n",
        "# We normalize only on training data to avoid leaking info from test set into training\n",
        "\n",
        "# Reshape 3D tensor to 2D: (windows, features) so scaler can fit\n",
        "train_reshaped = train_tensor.reshape(train_tensor.shape[0], -1)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_reshaped)  # Fit scaler only on training data\n",
        "\n",
        "# Transform train and test data with the same scaler parameters\n",
        "train_norm = scaler.transform(train_reshaped).reshape(train_tensor.shape)\n",
        "test_reshaped = test_tensor.reshape(test_tensor.shape[0], -1)\n",
        "test_norm = scaler.transform(test_reshaped).reshape(test_tensor.shape)\n",
        "\n",
        "# Replace original tensors with normalized versions\n",
        "train_tensor = train_norm\n",
        "test_tensor = test_norm\n",
        "\n",
        "# ------------------- Dataset with Sliding Window -------------------\n",
        "class WindowForecastDS(Dataset):\n",
        "    def __init__(self, tensor, seq_len):\n",
        "        self.tensor = tensor\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of samples is total windows minus sequence length\n",
        "        return len(self.tensor) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Input: seq_len consecutive windows\n",
        "        x = self.tensor[idx : idx + self.seq_len]\n",
        "        # Target: the window immediately following the input sequence\n",
        "        y = self.tensor[idx + self.seq_len]\n",
        "        return torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "# ------------------- DataLoader for Training and Testing -------------------\n",
        "train_dl = DataLoader(WindowForecastDS(train_tensor, SEQ_LEN_IN),\n",
        "                      batch_size=32, shuffle=True)   # Shuffle train data for stochasticity\n",
        "\n",
        "test_dl = DataLoader(WindowForecastDS(test_tensor, SEQ_LEN_IN),\n",
        "                     batch_size=32, shuffle=False)   # Do NOT shuffle test data to preserve order\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dl.dataset)}\")\n",
        "print(f\"Number of testing samples: {len(test_dl.dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8665e4f",
      "metadata": {
        "id": "f8665e4f"
      },
      "outputs": [],
      "source": [
        "class DASTBlock(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.temp = nn.Conv2d(d, d, (3,1), padding=(1,0), groups=d)\n",
        "        self.ag   = nn.Linear(d, d)\n",
        "        self.act  = nn.ReLU()\n",
        "    def forward(self, x):              # (B,T,N,d)\n",
        "        b,t,n,d = x.shape\n",
        "        x = x.permute(0,3,1,2)         # B,d,T,N\n",
        "        x = self.temp(x).permute(0,2,3,1)\n",
        "        x = self.ag(x)\n",
        "        return self.act(x)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DASTForecaster(nn.Module):\n",
        "    def __init__(self, n_roi, d=64, k=2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n_roi (int): Number of ROIs (regions of interest).\n",
        "            d (int): Hidden dimension size.\n",
        "            k (int): Number of DASTBlocks stacked.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_roi = n_roi\n",
        "        self.inp = nn.Linear(n_roi, d)\n",
        "        self.blks = nn.ModuleList([DASTBlock(d) for _ in range(k)])\n",
        "        self.out = nn.Linear(d, n_roi)  # Output predicts one row of the FC matrix\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (B, T, N, N), where\n",
        "                              B=batch size, T=timesteps, N=n_roi.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Predicted FC matrices of shape (B, n_roi, n_roi).\n",
        "        \"\"\"\n",
        "        assert x.dim() == 4, f\"Input must be 4D tensor (B,T,N,N), got {x.shape}\"\n",
        "        b, t, n, n2 = x.shape\n",
        "        assert n == self.n_roi and n2 == self.n_roi, f\"Input spatial dims must be n_roi ({self.n_roi}), got ({n}, {n2})\"\n",
        "        x = (x - x.mean()) / x.std()  # standardize globally or per sample\n",
        "\n",
        "        x = torch.tanh(x)  # Map values back to [-1, 1] range\n",
        "\n",
        "        # Reshape to (B*T*N, N) for linear input\n",
        "        x = x.view(b * t, n, n)           # (B*T, N, N)\n",
        "        x = self.inp(x)                   # (B*T, N, d)\n",
        "        x = x.view(b, t, n, -1)           # (B, T, N, d)\n",
        "\n",
        "        # Pass through stacked DASTBlocks\n",
        "        for blk in self.blks:\n",
        "            x = blk(x)  # each block expects (B, T, N, d)\n",
        "\n",
        "        # Temporal mean pooling over T dimension\n",
        "        x = x.mean(dim=1)  # (B, N, d)\n",
        "\n",
        "        x = self.out(x)  # (B, N, n_roi)\n",
        "\n",
        "        # Permute to (B, n_roi, n_roi) for output FC matrix\n",
        "        x = x.permute(0, 2, 1)  # (B, n_roi, n_roi)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = DASTForecaster(N_ROI, d=64, k=2).to(DEVICE)\n",
        "opt   = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "mse   = nn.MSELoss()\n",
        "best  = 1e9\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dca6b5e",
      "metadata": {
        "id": "6dca6b5e"
      },
      "outputs": [],
      "source": [
        "# Split train_tensor into train and validation sets (e.g. 80% train and 20% validation)\n",
        "val_ratio = 0.2\n",
        "val_size = int(len(train_tensor) * val_ratio)\n",
        "train_size = len(train_tensor) - val_size\n",
        "\n",
        "train_data = train_tensor[:train_size]\n",
        "val_data = train_tensor[train_size:]\n",
        "\n",
        "train_dl = DataLoader(WindowForecastDS(train_data, SEQ_LEN_IN), batch_size=BATCH, shuffle=True)\n",
        "val_dl = DataLoader(WindowForecastDS(val_data, SEQ_LEN_IN), batch_size=BATCH, shuffle=False)\n",
        "\n",
        "# EarlyStopping parameters\n",
        "patience = 5          # Number of epochs we wait before stopping if there is no improvement\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    running_train = 0\n",
        "    for xb, yb in tqdm(train_dl, desc=f\"Epoch {epoch:02d} - Training\", leave=False):\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        opt.zero_grad()\n",
        "        loss = mse(model(xb), yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running_train += loss.item() * xb.size(0)\n",
        "    train_mse = running_train / len(train_dl.dataset) / N_ROI**2\n",
        "    print(f\"Epoch {epoch:02d} | Train MSE/edge = {train_mse:.6f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    running_val = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_dl:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            val_pred = model(xb)\n",
        "            loss_val = mse(val_pred, yb)\n",
        "            running_val += loss_val.item() * xb.size(0)\n",
        "    val_mse = running_val / len(val_dl.dataset) / N_ROI**2\n",
        "    print(f\"Epoch {epoch:02d} | Validation MSE/edge = {val_mse:.6f}\")\n",
        "\n",
        "    # EarlyStopping check\n",
        "    if val_mse < best_val_loss:\n",
        "        best_val_loss = val_mse\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), \"best_dast_forecaster.pt\")\n",
        "        print(\"Validation loss improved, model saved.\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"No improvement in validation loss for {epochs_no_improve} epochs.\")\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3a5c2e2",
      "metadata": {
        "id": "f3a5c2e2"
      },
      "outputs": [],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Lists to store predictions, true targets, and RMSEs for each sliding window\n",
        "preds = []\n",
        "targets = []\n",
        "errors = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Loop over all possible sliding windows in the test data\n",
        "    for i in range(len(test_tensor) - SEQ_LEN_IN):\n",
        "        # Extract context sequence of length SEQ_LEN_IN starting from index i\n",
        "        test_context = torch.from_numpy(test_tensor[i:i+SEQ_LEN_IN][None, ...]).float().to(DEVICE)\n",
        "\n",
        "        # The ground-truth future adjacency matrix (the target we want to predict)\n",
        "        true_target = torch.from_numpy(test_tensor[i+SEQ_LEN_IN][None, ...]).float().to(DEVICE)\n",
        "\n",
        "        # Predict the next adjacency matrix based on the context\n",
        "        pred = model(test_context)\n",
        "\n",
        "        # Compute RMSE per edge for this prediction\n",
        "        err = torch.sqrt(mse(pred, true_target) / N_ROI**2).item()\n",
        "\n",
        "        # Store predicted and true values (converted back to NumPy)\n",
        "        preds.append(pred.cpu().numpy()[0])\n",
        "        targets.append(true_target.cpu().numpy()[0])\n",
        "        errors.append(err)\n",
        "\n",
        "# Stack all predicted and true adjacency matrices into arrays\n",
        "preds = np.stack(preds)\n",
        "targets = np.stack(targets)\n",
        "\n",
        "# Save all predictions and corresponding ground-truths for future analysis\n",
        "np.save(\"predicted_Y3_all_windows.npy\", preds)\n",
        "np.save(\"true_Y3_all_windows.npy\", targets)\n",
        "\n",
        "# Print average RMSE per edge across all test windows\n",
        "print(f\"Average RMSE/edge over test windows: {np.mean(errors):.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6599dafd",
      "metadata": {
        "id": "6599dafd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
