{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d76bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd15011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75 - Loss: 324.4204\n",
      "Epoch 2/75 - Loss: 322.8572\n",
      "Epoch 3/75 - Loss: 321.3033\n",
      "Epoch 4/75 - Loss: 319.7749\n",
      "Epoch 5/75 - Loss: 318.1318\n",
      "Epoch 6/75 - Loss: 316.3667\n",
      "Epoch 7/75 - Loss: 314.4029\n",
      "Epoch 8/75 - Loss: 312.2045\n",
      "Epoch 9/75 - Loss: 309.6205\n",
      "Epoch 10/75 - Loss: 306.7607\n",
      "Epoch 11/75 - Loss: 303.3496\n",
      "Epoch 12/75 - Loss: 299.3836\n",
      "Epoch 13/75 - Loss: 294.9559\n",
      "Epoch 14/75 - Loss: 289.6105\n",
      "Epoch 15/75 - Loss: 283.5974\n",
      "Epoch 16/75 - Loss: 276.6620\n",
      "Epoch 17/75 - Loss: 269.1977\n",
      "Epoch 18/75 - Loss: 260.6860\n",
      "Epoch 19/75 - Loss: 251.4237\n",
      "Epoch 20/75 - Loss: 241.6590\n",
      "Epoch 21/75 - Loss: 231.1267\n",
      "Epoch 22/75 - Loss: 220.6202\n",
      "Epoch 23/75 - Loss: 209.6255\n",
      "Epoch 24/75 - Loss: 198.1974\n",
      "Epoch 25/75 - Loss: 187.7386\n",
      "Epoch 26/75 - Loss: 177.0319\n",
      "Epoch 27/75 - Loss: 165.6129\n",
      "Epoch 28/75 - Loss: 155.2483\n",
      "Epoch 29/75 - Loss: 145.6849\n",
      "Epoch 30/75 - Loss: 136.6215\n",
      "Epoch 31/75 - Loss: 127.1458\n",
      "Epoch 32/75 - Loss: 119.0014\n",
      "Epoch 33/75 - Loss: 111.9290\n",
      "Epoch 34/75 - Loss: 103.7335\n",
      "Epoch 35/75 - Loss: 97.4939\n",
      "Epoch 36/75 - Loss: 91.1197\n",
      "Epoch 37/75 - Loss: 86.0933\n",
      "Epoch 38/75 - Loss: 80.0651\n",
      "Epoch 39/75 - Loss: 76.2511\n",
      "Epoch 40/75 - Loss: 71.2293\n",
      "Epoch 41/75 - Loss: 67.4425\n",
      "Epoch 42/75 - Loss: 63.5757\n",
      "Epoch 43/75 - Loss: 60.2711\n",
      "Epoch 44/75 - Loss: 57.0399\n",
      "Epoch 45/75 - Loss: 54.2722\n",
      "Epoch 46/75 - Loss: 51.7515\n",
      "Epoch 47/75 - Loss: 49.1022\n",
      "Epoch 48/75 - Loss: 46.9288\n",
      "Epoch 49/75 - Loss: 44.8028\n",
      "Epoch 50/75 - Loss: 43.0394\n",
      "Epoch 51/75 - Loss: 41.2039\n",
      "Epoch 52/75 - Loss: 39.7793\n",
      "Epoch 53/75 - Loss: 38.0439\n",
      "Epoch 54/75 - Loss: 37.0860\n",
      "Epoch 55/75 - Loss: 35.7137\n",
      "Epoch 56/75 - Loss: 34.6154\n",
      "Epoch 57/75 - Loss: 33.4129\n",
      "Epoch 58/75 - Loss: 32.3736\n",
      "Epoch 59/75 - Loss: 31.5365\n",
      "Epoch 60/75 - Loss: 30.6114\n",
      "Epoch 61/75 - Loss: 29.8630\n",
      "Epoch 62/75 - Loss: 29.1447\n",
      "Epoch 63/75 - Loss: 28.6238\n",
      "Epoch 64/75 - Loss: 27.9404\n",
      "Epoch 65/75 - Loss: 27.2406\n",
      "Epoch 66/75 - Loss: 26.8793\n",
      "Epoch 67/75 - Loss: 26.2338\n",
      "Epoch 68/75 - Loss: 25.8161\n",
      "Epoch 69/75 - Loss: 25.3161\n",
      "Epoch 70/75 - Loss: 25.0989\n",
      "Epoch 71/75 - Loss: 24.7175\n",
      "Epoch 72/75 - Loss: 24.2683\n",
      "Epoch 73/75 - Loss: 24.0720\n",
      "Epoch 74/75 - Loss: 23.7082\n",
      "Epoch 75/75 - Loss: 23.4543\n",
      "Epoch 76/75 - Loss: 23.3417\n",
      "Epoch 77/75 - Loss: 22.9809\n",
      "Epoch 78/75 - Loss: 22.6957\n",
      "Epoch 79/75 - Loss: 22.5703\n",
      "Epoch 80/75 - Loss: 22.4554\n",
      "Epoch 81/75 - Loss: 22.2475\n",
      "Epoch 82/75 - Loss: 22.0719\n",
      "Epoch 83/75 - Loss: 21.9211\n",
      "Epoch 84/75 - Loss: 21.8205\n",
      "Epoch 85/75 - Loss: 21.6835\n",
      "Epoch 86/75 - Loss: 21.4790\n",
      "Epoch 87/75 - Loss: 21.4116\n",
      "Epoch 88/75 - Loss: 21.3614\n",
      "Epoch 89/75 - Loss: 21.1524\n",
      "Epoch 90/75 - Loss: 21.1441\n",
      "Epoch 91/75 - Loss: 21.0755\n",
      "Epoch 92/75 - Loss: 20.9662\n",
      "Epoch 93/75 - Loss: 20.8235\n",
      "Epoch 94/75 - Loss: 20.7674\n",
      "Epoch 95/75 - Loss: 20.7134\n",
      "Epoch 96/75 - Loss: 20.6549\n",
      "Epoch 97/75 - Loss: 20.6034\n",
      "Epoch 98/75 - Loss: 20.5348\n",
      "Epoch 99/75 - Loss: 20.4737\n",
      "Epoch 100/75 - Loss: 20.3738\n",
      "Epoch 101/75 - Loss: 20.3301\n",
      "Epoch 102/75 - Loss: 20.3637\n",
      "Epoch 103/75 - Loss: 20.2786\n",
      "Epoch 104/75 - Loss: 20.1652\n",
      "Epoch 105/75 - Loss: 20.1320\n",
      "Epoch 106/75 - Loss: 20.0912\n",
      "Epoch 107/75 - Loss: 20.0429\n",
      "Epoch 108/75 - Loss: 19.9818\n",
      "Epoch 109/75 - Loss: 19.9459\n",
      "Epoch 110/75 - Loss: 19.9248\n",
      "Epoch 111/75 - Loss: 19.8559\n",
      "Epoch 112/75 - Loss: 19.8506\n",
      "Epoch 113/75 - Loss: 19.7788\n",
      "Epoch 114/75 - Loss: 19.7254\n",
      "Epoch 115/75 - Loss: 19.6877\n",
      "Epoch 116/75 - Loss: 19.6971\n",
      "Epoch 117/75 - Loss: 19.5985\n",
      "Epoch 118/75 - Loss: 19.5621\n",
      "Epoch 119/75 - Loss: 19.5607\n",
      "Epoch 120/75 - Loss: 19.4807\n",
      "Epoch 121/75 - Loss: 19.4488\n",
      "Epoch 122/75 - Loss: 19.4005\n",
      "Epoch 123/75 - Loss: 19.3566\n",
      "Epoch 124/75 - Loss: 19.3189\n",
      "Epoch 125/75 - Loss: 19.2732\n",
      "Epoch 126/75 - Loss: 19.2305\n",
      "Epoch 127/75 - Loss: 19.1848\n",
      "Epoch 128/75 - Loss: 19.1558\n",
      "Epoch 129/75 - Loss: 19.1047\n",
      "Epoch 130/75 - Loss: 19.0732\n",
      "Epoch 131/75 - Loss: 19.0165\n",
      "Epoch 132/75 - Loss: 18.9691\n",
      "Epoch 133/75 - Loss: 18.9284\n",
      "Epoch 134/75 - Loss: 18.9114\n",
      "Epoch 135/75 - Loss: 18.8415\n",
      "Epoch 136/75 - Loss: 18.7981\n",
      "Epoch 137/75 - Loss: 18.7521\n",
      "Epoch 138/75 - Loss: 18.6977\n",
      "Epoch 139/75 - Loss: 18.6457\n",
      "Epoch 140/75 - Loss: 18.6393\n",
      "Epoch 141/75 - Loss: 18.5404\n",
      "Epoch 142/75 - Loss: 18.5072\n",
      "Epoch 143/75 - Loss: 18.4358\n",
      "Epoch 144/75 - Loss: 18.3910\n",
      "Epoch 145/75 - Loss: 18.3426\n",
      "Epoch 146/75 - Loss: 18.2862\n",
      "Epoch 147/75 - Loss: 18.2365\n",
      "Epoch 148/75 - Loss: 18.1700\n",
      "Epoch 149/75 - Loss: 18.1188\n",
      "Epoch 150/75 - Loss: 18.0416\n",
      "Epoch 151/75 - Loss: 17.9841\n",
      "Epoch 152/75 - Loss: 17.9177\n",
      "Epoch 153/75 - Loss: 17.8688\n",
      "Epoch 154/75 - Loss: 17.8143\n",
      "Epoch 155/75 - Loss: 17.7242\n",
      "Epoch 156/75 - Loss: 17.6385\n",
      "Epoch 157/75 - Loss: 17.5727\n",
      "Epoch 158/75 - Loss: 17.5034\n",
      "Epoch 159/75 - Loss: 17.4126\n",
      "Epoch 160/75 - Loss: 17.3513\n",
      "Epoch 161/75 - Loss: 17.2598\n",
      "Epoch 162/75 - Loss: 17.1779\n",
      "Epoch 163/75 - Loss: 17.1025\n",
      "Epoch 164/75 - Loss: 17.0204\n",
      "Epoch 165/75 - Loss: 16.9195\n",
      "Epoch 166/75 - Loss: 16.8260\n",
      "Epoch 167/75 - Loss: 16.7125\n",
      "Epoch 168/75 - Loss: 16.6209\n",
      "Epoch 169/75 - Loss: 16.5146\n",
      "Epoch 170/75 - Loss: 16.4371\n",
      "Epoch 171/75 - Loss: 16.3075\n",
      "Epoch 172/75 - Loss: 16.1998\n",
      "Epoch 173/75 - Loss: 16.0674\n",
      "Epoch 174/75 - Loss: 15.9702\n",
      "Epoch 175/75 - Loss: 15.8398\n",
      "Epoch 176/75 - Loss: 15.7497\n",
      "Epoch 177/75 - Loss: 15.5939\n",
      "Epoch 178/75 - Loss: 15.4476\n",
      "Epoch 179/75 - Loss: 15.3087\n",
      "Epoch 180/75 - Loss: 15.1452\n",
      "Epoch 181/75 - Loss: 15.0298\n",
      "Epoch 182/75 - Loss: 14.8769\n",
      "Epoch 183/75 - Loss: 14.6816\n",
      "Epoch 184/75 - Loss: 14.5127\n",
      "Epoch 185/75 - Loss: 14.3228\n",
      "Epoch 186/75 - Loss: 14.1547\n",
      "Epoch 187/75 - Loss: 13.9914\n",
      "Epoch 188/75 - Loss: 13.7404\n",
      "Epoch 189/75 - Loss: 13.5655\n",
      "Epoch 190/75 - Loss: 13.4007\n",
      "Epoch 191/75 - Loss: 13.1842\n",
      "Epoch 192/75 - Loss: 13.0418\n",
      "Epoch 193/75 - Loss: 12.8771\n",
      "Epoch 194/75 - Loss: 12.7522\n",
      "Epoch 195/75 - Loss: 12.4687\n",
      "Epoch 196/75 - Loss: 12.2952\n",
      "Epoch 197/75 - Loss: 12.1641\n",
      "Epoch 198/75 - Loss: 11.8756\n",
      "Epoch 199/75 - Loss: 11.6789\n",
      "Epoch 200/75 - Loss: 11.5016\n",
      "\n",
      "✅ Final Results → RMSE: 8.0125, R²: -1.1820\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load tensors\n",
    "X = np.load(\"X_tensor.npy\")  # shape: (N, 3, E)\n",
    "Y = np.load(\"Y_tensor.npy\")  # shape: (N, 3)\n",
    "subject_ids = np.load(\"subject_ids.npy\")\n",
    "\n",
    "\n",
    "X_input = X[:, :2, :]    # (N, 2, E)\n",
    "Y_target = Y[:, 2]       # (N,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fb4e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientTrajectoryDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.Y = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "# Split\n",
    "dataset = PatientTrajectoryDataset(X_input, Y_target)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size  = len(dataset) - train_size\n",
    "train_ds, test_ds = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e723137",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple DAST-GCN-inspired model\n",
    "class DASTGCNSimple(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64):\n",
    "        super(DASTGCNSimple, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=num_features, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc   = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, time, features)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        out = self.fc(h_n[-1])  # last hidden state\n",
    "        return out.squeeze()\n",
    "\n",
    "\n",
    "model = DASTGCNSimple(num_features=X_input.shape[2])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d79f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/75 - Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        pred = model(xb)\n",
    "        y_true.extend(yb.numpy())\n",
    "        y_pred.extend(pred.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a7b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2   = r2_score(y_true, y_pred)\n",
    "print(f\"\\n✅ Final Results → RMSE: {rmse:.4f}, R²: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
